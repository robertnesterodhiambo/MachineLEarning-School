{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "\n",
    "Retail companies, in their pursuit of unraveling the intricacies of customer behavior, have been striving to understand consumption patterns and improve predictive capabilities. In this endeavor, my primary objective is to develop a comprehensive predictive model that delves into the diverse customer base, anticipating the specific products they are likely to purchase. Utilizing a robust dataset encompassing extensive customer details, product information, and historical order data, the goal is to discern nuanced patterns and trends that drive purchasing decisions. The predictive model will intricately incorporate various factors, including customer demographics, order history, and product attributes. To elevate the model's performance, sophisticated feature engineering techniques will be explored, ensuring the creation of pertinent features that contribute to enhanced prediction accuracy. The overarching ambition is to furnish actionable insights empowering marketing and sales teams with targeted product recommendations, optimizing customer satisfaction and maximizing sales opportunities. This predictive analytics endeavor stands as a pivotal step toward unlocking the full potential of business intelligence, fostering informed decision-making, and driving sustained growth in a dynamic market landscape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331d563",
   "metadata": {},
   "source": [
    "**Research Questions:**\n",
    "\n",
    "1. **RQ1: Holistic Customer Segmentation and Demographics Analysis**\n",
    "   - How can I develop a comprehensive customer segmentation strategy, considering a diverse range of demographic, psychographic, and behavioral characteristics?\n",
    "   - What influence do cultural and regional factors have on my customers' preferences, and how can I incorporate these nuances into my segmentation strategies?\n",
    "\n",
    "2. **RQ2: In-depth Exploration of My Consumption Patterns**\n",
    "   - What are the longitudinal aspects of my customer consumption patterns, and how do these evolve over time?\n",
    "   - How can I integrate machine learning algorithms to identify subtle patterns, anomalies, and outliers in my large-scale historical order data?\n",
    "\n",
    "3. **RQ3: Advancing My Predictive Modeling for Personalized Recommendations**\n",
    "   - In addition to traditional predictive modeling, how can I incorporate advanced techniques such as deep learning to achieve more nuanced and personalized product recommendations?\n",
    "   - What role do external factors like macroeconomic indicators and social trends play in enhancing the accuracy of my predictive models?\n",
    "\n",
    "4. **RQ4: Feature Engineering for Multi-dimensional Insights**\n",
    "   - How can I leverage not only my customer details, product information, and order history but also incorporate external data sources for comprehensive feature engineering?\n",
    "   - What techniques can I employ to assess the importance and interaction of various features in driving predictive accuracy?\n",
    "\n",
    "5. **RQ5: Actionable Insights Across My Organization**\n",
    "   - Beyond marketing and sales, how can actionable insights be extended to other touchpoints such as customer support and product development?\n",
    "   - What strategies can I implement to ensure seamless communication and application of insights across different departments within my organization?\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Objective 1: Integration of Multifaceted Customer Segmentation**\n",
    "   - I aim to develop a comprehensive framework for customer segmentation that considers a diverse set of demographic, psychographic, and behavioral factors.\n",
    "\n",
    "2. **Objective 2: Longitudinal Analysis of My Consumption Patterns**\n",
    "   - I will conduct an in-depth longitudinal analysis of my customer consumption patterns, exploring temporal variations and seasonality.\n",
    "\n",
    "3. **Objective 3: Integration of Advanced Predictive Modeling Techniques**\n",
    "   - I intend to explore and implement advanced predictive modeling techniques, including deep learning, to provide more personalized and accurate product recommendations.\n",
    "\n",
    "4. **Objective 4: Comprehensive Feature Engineering Framework**\n",
    "   - I am dedicated to developing a robust framework for feature engineering that incorporates not only traditional customer and product data but also external data sources for a multi-dimensional view.\n",
    "\n",
    "5. **Objective 5: Cross-Functional Application of Actionable Insights**\n",
    "   - I will establish protocols and communication channels for the dissemination and application of actionable insights across various departments, fostering a holistic organizational approach.\n",
    "\n",
    "6. **Objective 6: Real-time Adaptation and Continuous Improvement**\n",
    "   - I aim to implement mechanisms for real-time adaptation of strategies based on evolving consumer trends, ensuring continuous improvement in the accuracy and relevance of my predictive models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf1ff2",
   "metadata": {},
   "source": [
    "# Data cleaning and preparation\n",
    "\n",
    "# data imporation\n",
    "\n",
    "\n",
    "In the process of importing data into my Spark environment, I initiated a Spark session using the PySpark library, marking the beginning of my data processing journey. Specifying the path to the folder where my CSV files reside, I meticulously curated a list of these files, filtering only those with the '.csv' extension. This step ensured a focused approach to my data loading endeavor. Employing a dynamic and scalable strategy, I proceeded to iterate through each CSV file within the designated folder. For every iteration, I conscientiously extracted the file name (excluding the extension) and employed it as a variable name. This approach aimed at maintaining clarity and organization within my environment. With each iteration, I invoked Spark's `read.csv` method, reading the contents of the CSV file into a distinct DataFrame. These DataFrames were then not only appended to a list for a comprehensive overview but, more significantly, embedded into my environment using their respective file names as variable references. This meticulous orchestration enabled seamless access to each DataFrame by its designated variable, thereby facilitating subsequent analysis and exploration. The conclusive act of displaying the contents of each DataFrame using the `show()` method underscored the successful importation of diverse datasets, each now poised for further exploration and insightful analysis within my Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543c8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def load_csv_files_into_dataframes(folder_path):\n",
    "    \"\"\"\n",
    "    Load multiple CSV files into Spark DataFrames and save them in the environment with their file names.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    - List of Spark DataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder.appName(\"CSVLoaderFunction\").getOrCreate()\n",
    "\n",
    "    # Get a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # Create a list to store the DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Create variables for each DataFrame\n",
    "    for csv_file in csv_files:\n",
    "        # Use the file name (without extension) as the variable name\n",
    "        df_name = os.path.splitext(csv_file)[0]\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = spark.read.csv(os.path.join(folder_path, csv_file), header=True, inferSchema=True)\n",
    "        # Save the DataFrame in the environment with its file name\n",
    "        globals()[df_name] = df\n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Example usage:\n",
    "folder_path = r'C:\\Users\\neste\\OneDrive\\Desktop\\karanja\\DataSet_final\\DataSet_final'\n",
    "loaded_dataframes = load_csv_files_into_dataframes(folder_path)\n",
    "\n",
    "# Show the contents of each DataFrame\n",
    "DimGeography.show()\n",
    "DimAccount.show()\n",
    "DimCurrency.show()\n",
    "DimCustomer.show()\n",
    "DimDate.show()\n",
    "DimDepartmentGroup.show()\n",
    "DimOrganization.show()\n",
    "DimProduct.show()\n",
    "DimProductCategory.show()\n",
    "DimProductSubcategory.show()\n",
    "DimPromotion.show()\n",
    "DimReseller.show()\n",
    "DimSalesReason.show()\n",
    "DimSalesTerritory.show()\n",
    "DimScenario.show()\n",
    "FactCallCenter.show()\n",
    "FactCurrencyRate.show()\n",
    "FactInternetSales.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1483905",
   "metadata": {},
   "source": [
    "# data preparation\n",
    "\n",
    "## data joining \n",
    "\n",
    "In weaving together a comprehensive dataset for analysis, I embarked on a series of strategic joins, each adding a layer of richness to the information at hand. Initially, I sought to enhance the understanding of product-related data by merging the `FactInternetSales` data with `DimProduct` through a join on the common ground of `ProductKey`. This amalgamation aimed to facilitate a deeper exploration of product-centric insights, incorporating details such as product subcategories, names, and key attributes.\n",
    "\n",
    "Subsequently, my focus shifted to customer-centric insights. Through a join with `DimCustomer`, I seamlessly integrated customer-specific information into the evolving dataset, using the `CustomerKey` as the bridge. This strategic decision allowed for a holistic examination of customer behavior, encompassing demographics, purchase history, and other pertinent details.\n",
    "\n",
    "The inclusion of promotional details became the next logical step in unraveling the dynamics of sales. By merging with `DimPromotion` based on the shared `PromotionKey`, I introduced promotional aspects such as names, discount percentages, and categories. This step added a temporal dimension to the dataset, enabling a closer examination of the impact of promotions on sales patterns.\n",
    "\n",
    "Considering the financial dimension, the dataset expanded to include currency-related insights through a join with `DimCurrency` using the common identifier `CurrencyKey`. This addition allowed for a standardized representation of monetary values, enhancing the precision of financial analyses.\n",
    "\n",
    "Sales territory details were seamlessly incorporated into the dataset by joining with `DimSalesTerritory` using the `SalesTerritoryKey` as a linking element. This strategic inclusion provided geographical context to sales data, facilitating analyses related to regional trends, market performance, and customer distribution.\n",
    "\n",
    "To deepen the understanding of product subcategories, I orchestrated a join with `DimProductSubcategory`, utilizing the `ProductSubcategoryKey` as the connection point. This step allowed for a more nuanced exploration of product types and their categorizations, enabling insights into product preferences and market segments.\n",
    "\n",
    "Finally, a geographical layer was added to the dataset through a join with `DimGeography`, utilizing the `GeographyKey` as the amalgamating factor. This geographical context provided a framework for exploring regional variations, customer distribution across cities, and other spatial considerations.\n",
    "\n",
    "Each join in this sequential process was a deliberate choice aimed at creating a comprehensive and interconnected dataset. The rationale behind these joins was rooted in the pursuit of a holistic understanding of the business landscape. By integrating diverse dimensions such as products, customers, promotions, currencies, sales territories, and geography, the resulting dataset stands poised to offer nuanced insights, empowering data-driven decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9187f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, expr\n",
    "\n",
    "# Select the specific columns from DimProduct\n",
    "selected_columns = [\"ProductKey\", \"ProductSubcategoryKey\", \"EnglishProductName\", \"FinishedGoodsFlag\",\n",
    "                    \"Color\", \"SafetyStockLevel\", \"ReorderPoint\", \"SizeRange\", \"DaysToManufacture\"]\n",
    "\n",
    "dim_product_selected = DimProduct.select(selected_columns)\n",
    "\n",
    "# Perform the join operation on ProductKey\n",
    "prepared_data = FactInternetSales.join(dim_product_selected, \"ProductKey\")\n",
    "\n",
    "\n",
    "# Select the specific columns from DimCustomer, including DateFirstPurchase\n",
    "customer_selected_columns = [\"CustomerKey\", \"GeographyKey\", \"NameStyle\", \"BirthDate\", \"MaritalStatus\", \"Gender\",\n",
    "                              \"YearlyIncome\", \"TotalChildren\", \"NumberChildrenAtHome\", \"EnglishEducation\",\n",
    "                              \"EnglishOccupation\", \"HouseOwnerFlag\", \"NumberCarsOwned\", \"CommuteDistance\",\n",
    "                              \"DateFirstPurchase\"]\n",
    "\n",
    "dim_customer_selected = DimCustomer.select(customer_selected_columns)\n",
    "\n",
    "# Perform the join operation on CustomerKey\n",
    "prepared_data = prepared_data.join(dim_customer_selected, \"CustomerKey\")\n",
    "\n",
    "# Select the specific columns from DimPromotion\n",
    "promotion_selected_columns = [\"PromotionKey\", \"EnglishPromotionName\", \"DiscountPct\", \"EnglishPromotionCategory\", \"MinQty\"]\n",
    "\n",
    "dim_promotion_selected = DimPromotion.select(promotion_selected_columns)\n",
    "\n",
    "# Perform the join operation on PromotionKey with the existing prepared_data DataFrame\n",
    "prepared_data = prepared_data.join(dim_promotion_selected, \"PromotionKey\")\n",
    "\n",
    "\n",
    "# Select the specific column from DimCurrency\n",
    "currency_selected_columns = [\"CurrencyKey\", \"CurrencyName\"]\n",
    "\n",
    "dim_currency_selected = DimCurrency.select(currency_selected_columns)\n",
    "\n",
    "# Perform the join operation on CurrencyKey with the existing prepared_data DataFrame\n",
    "prepared_data = prepared_data.join(dim_currency_selected, \"CurrencyKey\")\n",
    "\n",
    "\n",
    "# Select the specific columns from DimSalesTerritory\n",
    "sales_territory_selected_columns = [\"SalesTerritoryKey\", \"SalesTerritoryRegion\", \"SalesTerritoryCountry\"]\n",
    "\n",
    "dim_sales_territory_selected = DimSalesTerritory.select(sales_territory_selected_columns)\n",
    "\n",
    "# Perform the join operation on SalesTerritoryKey with the existing prepared_data DataFrame\n",
    "prepared_data = prepared_data.join(dim_sales_territory_selected, \"SalesTerritoryKey\")\n",
    "\n",
    "\n",
    "\n",
    "# Select the specific columns from DimProductSubcategory\n",
    "product_subcategory_selected_columns = [\"ProductSubcategoryKey\", \"EnglishProductSubcategoryName\"]\n",
    "\n",
    "dim_product_subcategory_selected = DimProductSubcategory.select(product_subcategory_selected_columns)\n",
    "\n",
    "# Perform the join operation on ProductSubcategoryKey with the existing prepared_data DataFrame\n",
    "prepared_data = prepared_data.join(dim_product_subcategory_selected, \"ProductSubcategoryKey\")\n",
    "\n",
    "# Select the specific columns from DimGeography\n",
    "geography_selected_columns = [\"GeographyKey\", \"City\", \"StateProvinceName\"]\n",
    "\n",
    "dim_geography_selected = DimGeography.select(geography_selected_columns)\n",
    "\n",
    "# Perform the join operation on GeographyKey with the existing prepared_data DataFrame\n",
    "prepared_data = prepared_data.join(dim_geography_selected, \"GeographyKey\")\n",
    "\n",
    "# Convert the numeric representation to a string and then to a DateType\n",
    "prepared_data = prepared_data.withColumn(\n",
    "    \"OrderDate\",\n",
    "    to_date(expr(\"cast(OrderDateKey as string)\"), \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "prepared_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fdad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_date, expr, month, year\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, month, year, expr\n",
    "\n",
    "def perform_joins(fact_df: DataFrame, *dimension_dfs: DataFrame, join_keys: list, date_column: str = None):\n",
    "    \"\"\"\n",
    "    Perform a series of joins between a Fact DataFrame and multiple Dimension DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - fact_df: The Fact DataFrame to start with.\n",
    "    - dimension_dfs: A variable number of Dimension DataFrames to join with the Fact DataFrame.\n",
    "    - join_keys: A list of join keys to be used in the order of the joins.\n",
    "    - date_column: Optional. If provided, convert this numeric column to DateType.\n",
    "\n",
    "    Returns:\n",
    "    - The resulting DataFrame after all the specified joins.\n",
    "    \"\"\"\n",
    "    prepared_data = fact_df\n",
    "\n",
    "    for dimension_df, join_key in zip(dimension_dfs, join_keys):\n",
    "        prepared_data = prepared_data.join(dimension_df, join_key)\n",
    "\n",
    "    if date_column:\n",
    "        # Convert the numeric representation to a string and then to a DateType\n",
    "        prepared_data = prepared_data.withColumn(\n",
    "            \"OrderDate\",\n",
    "            to_date(expr(f\"cast({date_column} as string)\"), \"yyyyMMdd\")\n",
    "        )\n",
    "\n",
    "    return prepared_data\n",
    "\n",
    "# Example usage:\n",
    "prepared_data = perform_joins(\n",
    "    FactInternetSales,\n",
    "    DimProduct,\n",
    "    DimCustomer,\n",
    "    DimPromotion,\n",
    "    DimCurrency,\n",
    "    DimSalesTerritory,\n",
    "    DimProductSubcategory,\n",
    "    DimGeography,\n",
    "    join_keys=[\"ProductKey\", \"CustomerKey\", \"PromotionKey\", \"CurrencyKey\", \"SalesTerritoryKey\", \"ProductSubcategoryKey\", \"GeographyKey\"],\n",
    "    date_column=\"OrderDateKey\"\n",
    ")\n",
    "\n",
    "# Extract order month and order year\n",
    "prepared_data = prepared_data.withColumn(\"OrderMonth\", month(\"OrderDate\"))\n",
    "prepared_data = prepared_data.withColumn(\"OrderYear\", year(\"OrderDate\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "prepared_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87011891",
   "metadata": {},
   "source": [
    "# cleaned data \n",
    "In the refinement of our dataset, I have strategically excluded several columns that contribute limited value to our specific analytical goals. These excluded columns encompass various keys, identifiers, and descriptive attributes that, while potentially valuable in other contexts, are deemed extraneous for our current data exploration objectives.\n",
    "\n",
    "This meticulous exclusion process aims to enhance the precision and relevance of our dataset, creating a more focused DataFrame named \"cleaned_data.\" By eliminating non-essential information, we optimize the dataset for subsequent analyses, ensuring that the retained data aligns closely with the specific insights we seek to derive. This precision-driven approach not only streamlines the dataset but also facilitates a more efficient and meaningful exploration, enhancing the potential for uncovering valuable patterns and trends pertinent to our analytical objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b88e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = [\n",
    "    \"GeographyKey\", \"ProductSubcategoryKey\", \"SalesTerritoryKey\", \"CurrencyKey\", \"PromotionKey\",\n",
    "    \"CustomerKey\", \"ProductKey\", \"OrderDateKey\", \"DueDateKey\", \"ShipDateKey\", \"SalesOrderNumber\",\n",
    "    \"SalesOrderLineNumber\", \"RevisionNumber\", \"OrderQuantity\", \"CarrierTrackingNumber\",\n",
    "    \"CustomerPONumber\", \"OrderDate\", \"DueDate\", \"ShipDate\", \"ProductAlternateKey\",\n",
    "    \"SpanishProductName\", \"FrenchProductName\", \"EnglishDescription\", \"StartDate\", \"EndDate\", \"Status\",\n",
    "    \"CustomerAlternateKey\", \"Title\", \"FirstName\", \"MiddleName\", \"LastName\", \"BirthDate\", \"EmailAddress\",\n",
    "    \"SpanishEducation\", \"FrenchEducation\", \"EnglishOccupation\", \"SpanishOccupation\", \"FrenchOccupation\",\n",
    "    \"AddressLine1\", \"AddressLine2\", \"Phone\", \"DateFirstPurchase\", \"SpanishPromotionName\",\n",
    "    \"FrenchPromotionName\", \"EnglishPromotionType\", \"SpanishPromotionType\", \"FrenchPromotionType\",\n",
    "    \"SpanishPromotionCategory\", \"FrenchPromotionCategory\", \"StartDate\", \"EndDate\", \"MaxQty\",\n",
    "    \"CurrencyAlternateKey\", \"ProductCategoryKey\", \"StateProvinceCode\", \"CountryRegionCode\",\n",
    "    \"SpanishCountryRegionName\", \"FrenchCountryRegionName\", \"PostalCode\", \"SalesTerritoryKey\",\n",
    "    \"IpAddressLocator\", \"SalesTerritoryAlternateKey\", \"SalesTerritoryRegion\",\n",
    "    \"ProductSubcategoryAlternateKey\",\"Suffix\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns from the DataFrame and assign the result to \"cleaned_data\"\n",
    "cleaned_data = prepared_data.drop(*columns_to_exclude)\n",
    "\n",
    "# Show the resulting cleaned DataFrame\n",
    "cleaned_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8885ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2bfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Specify the categorical columns to convert\n",
    "categorical_columns = [\"MaritalStatus\", \"Gender\", \"CommuteDistance\", \n",
    "                        \"EnglishEducation\", \"EnglishPromotionName\", \"CurrencyName\", \n",
    "                        \"SalesTerritoryRegion\", \"SalesTerritoryCountry\", \"SalesTerritoryGroup\", \n",
    "                        \"StateProvinceName\", \"EnglishCountryRegionName\", \"City\"]\n",
    "\n",
    "# Identify non-categorical columns\n",
    "non_categorical_columns = [col for col in cleaned_data.columns if col not in categorical_columns]\n",
    "\n",
    "# Convert boolean columns to string\n",
    "boolean_columns = [col for col in categorical_columns if cleaned_data.schema[col].dataType == 'boolean']\n",
    "for boolean_col in boolean_columns:\n",
    "    cleaned_data = cleaned_data.withColumn(boolean_col, col(boolean_col).cast(\"string\"))\n",
    "\n",
    "# Create StringIndexers for text columns\n",
    "string_indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\") for col in categorical_columns if cleaned_data.schema[col].dataType == 'string']\n",
    "\n",
    "# Create a pipeline to apply the StringIndexers\n",
    "string_indexer_pipeline = Pipeline(stages=string_indexers)\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "transformed_data = string_indexer_pipeline.fit(cleaned_data).transform(cleaned_data)\n",
    "\n",
    "# Create OneHotEncoders for indexed columns\n",
    "one_hot_encoders = [OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_encoded\") for col in categorical_columns if cleaned_data.schema[col].dataType == 'string']\n",
    "\n",
    "# Create a pipeline to apply the OneHotEncoders\n",
    "one_hot_encoder_pipeline = Pipeline(stages=one_hot_encoders)\n",
    "\n",
    "# Fit and transform the pipeline\n",
    "transformed_data = one_hot_encoder_pipeline.fit(transformed_data).transform(transformed_data)\n",
    "\n",
    "# Select non-categorical columns and the encoded columns\n",
    "selected_columns = non_categorical_columns + [f\"{col}_encoded\" for col in categorical_columns if cleaned_data.schema[col].dataType == 'string']\n",
    "transformed_data = transformed_data.select(selected_columns)\n",
    "\n",
    "# Show the result\n",
    "transformed_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9ceea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
